{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKWFTbHpyBLX"
      },
      "source": [
        "import numpy as np\n",
        "from sympy import *"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo-G-zQgl08g"
      },
      "source": [
        "##Gradient Descent Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHU4uUEBxXkx",
        "outputId": "06e1da71-8894-40a0-b22f-d3807f44cd6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#defining the variables x and y\n",
        "x,y = symbols('x y')\n",
        "\n",
        "#define one variable function to optimize\n",
        "f = 3*x**3 - 9*x\n",
        "\n",
        "#define derivative of the function\n",
        "fprime = f.diff(x)\n",
        "\n",
        "#evaluate the derivative at x = 5\n",
        "fprime.evalf(subs={x: 5})"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "216.000000000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZBngcjRx6VP",
        "outputId": "a8e92069-2f3b-46fb-b0c8-80eb8859210e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def gradient_descent(start_point, learning_rate, max_it=1000, tol=1e-7, print_steps=False):\n",
        "  \"\"\"\n",
        "  Returns minimum of the function f and iteration number by applying gradient descent optimization algorithm\n",
        "  \n",
        "  Gradient descent is an optimization algorithm that uses first order derivative of the function.\n",
        "  By providing 4 main parameters, the algorithm should be able to find the minimum. This function \n",
        "  is suitable for one variable functions.\n",
        "\n",
        "  Parameters\n",
        "\n",
        "  start_point: float32, initial point for gradient descent to start\n",
        "  \n",
        "  learning_rate: float32, step-size/alpha hyper-parameter to use for gradient descent algorithm\n",
        "\n",
        "  max_it: default 1000, uint32, maximum iteration number for optimization algorithm to perform\n",
        "          if the maximum iteration is exceeded, the algorithm stops\n",
        "\n",
        "  tol: default 1e-7, float32, tolerance for stopping criteria. If the change in the update is less than \n",
        "        tolerance specified, the algorithm stops.\n",
        "\n",
        "  print_steps: default False, prints the change and iteration number at each iteration if True.\n",
        "  \"\"\"\n",
        "  k = 0             #Iteration number\n",
        "  w_prev = 0        #previous point w to calculate change\n",
        "  w = start_point   #current minimum point w to return\n",
        "\n",
        "  while k <= max_it and np.abs(w-w_prev) > tol:\n",
        "    w_prev = w\n",
        "\n",
        "    w = w - learning_rate * fprime.evalf(subs={x: w})\n",
        "\n",
        "    k = k + 1\n",
        "    \n",
        "    if print_steps:\n",
        "      print(\"Iteration \", k, \" w: \", w)\n",
        "  \n",
        "  return w, k\n",
        "\n",
        "min, it = gradient_descent(8, 0.01)\n",
        "print(\"Minimum of function \", f, \" is at point x = \", min, \" in \", it, \" iterations.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum of function  3*x**3 - 9*x  is at point x =  1.00000044296222  in  73  iterations.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA_f5atZlvMm"
      },
      "source": [
        "##Newtons method for optimizing multivariate function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JupxBLT0cr-"
      },
      "source": [
        "## Initializing second function with variables x and y\n",
        "g = y**3 + x**2 - 6*x - 6*y\n",
        "\n",
        "## Initialize starting point for newton method\n",
        "start = np.array([[56], [873]])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Elpykf_ztoV",
        "outputId": "57721f5a-6ab9-435b-aa65-ef5b4a0bac0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def newtons_method(start_point, max_it=1000, tol=1e-7, print_steps=False):\n",
        "  \"\"\"\n",
        "  Returns minimum of the function f and iteration number by applying newtons method optimization algorithm\n",
        "  \n",
        "  Newtons method uses 2nd order approximation to find the minimum of the convex function. \n",
        "  Compared to Gradient Descent, it takes one less parameter, which is learning rate/alpha \n",
        "  hyper-parameter. \n",
        "\n",
        "  Parameters\n",
        "\n",
        "  start_point: 1D ndarray, initial point for Newton's method to start\n",
        "\n",
        "  max_it: default 1000, uint32, maximum iteration number for optimization algorithm to perform\n",
        "          if the maximum iteration is exceeded, the algorithm stops\n",
        "\n",
        "  tol: default 1e-7, float32, tolerance for stopping criteria. If the change in the update is less than \n",
        "        tolerance specified, the algorithm stops.\n",
        "  \n",
        "  print_steps: default False, prints the change and iteration number at each iteration if True.\n",
        "  \"\"\"\n",
        "\n",
        "  k = 0                           #iteration number\n",
        "  w_prev = np.random.rand(2,1)    #previous point to calculate the update difference\n",
        "  w = start_point                 #starting point\n",
        "\n",
        "  while k <= max_it and np.linalg.norm(w-w_prev) > tol:\n",
        "    w_prev = w\n",
        "\n",
        "    ## Calculating the gradient vector by taking derivatives of g with respect to x and y\n",
        "    gx = g.diff(x).evalf(subs={x: w[0,0], y: w[1,0]})\n",
        "    gy = g.diff(y).evalf(subs={x: w[0,0], y: w[1,0]})\n",
        "    gradient = np.array([[gx],[gy]], dtype=\"float\")\n",
        "\n",
        "    ## Calculate the Hessian matrix\n",
        "    gxx = g.diff(x,x).evalf(subs={x: w[0,0], y: w[1,0]})\n",
        "    gxy = g.diff(x,y).evalf(subs={x: w[0,0], y: w[1,0]})\n",
        "    gyx = g.diff(y,x).evalf(subs={x: w[0,0], y: w[1,0]})\n",
        "    gyy = g.diff(y,y).evalf(subs={x: w[0,0], y: w[1,0]})\n",
        "    hessian = np.array([[gxx, gxy],[gyx, gyy]], dtype=\"float\")\n",
        "\n",
        "    ## Updating the point w\n",
        "    w = w - np.dot(np.linalg.inv(hessian), gradient)\n",
        "\n",
        "    ## Increment the iteration number\n",
        "    k = k + 1\n",
        "\n",
        "    if print_steps:\n",
        "      print(\"Iteration \", k, \" w: \", w)\n",
        "\n",
        "  return w, k\n",
        "\n",
        "min, it = newtons_method(start)\n",
        "print(\"Minimum of function \", g, \" is at point (x,y) = \", min, \" found in \", it, \" iterations.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum of function  x**2 - 6*x + y**3 - 6*y  is at point (x,y) =  [[3.        ]\n",
            " [1.41421356]]  found in  14  iterations.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}